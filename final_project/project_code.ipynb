{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, url_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Project on Information Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_read(filename, encoding='utf-8'):\n",
    "    with open(filename, 'r', encoding=encoding) as inf:\n",
    "        res = json.load(inf)\n",
    "        return res\n",
    "\n",
    "def json_dump(obj, filename, ea=False, indent=4, encoding='utf-8'):\n",
    "    with open(filename, 'w', encoding=encoding) as ouf:\n",
    "        json.dump(obj, ouf, ensure_ascii=ea, indent=indent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading raw data for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./quora_question_pairs_rus.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [item for item in list(data['question1']) if type(item) != float]\n",
    "corpus.extend([item for item in list(data['question2']) if type(item) != float])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to get lemmas and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def normalize(doc):\n",
    "    tokens = re.sub('  ', ' ', re.sub(r'[^\\w\\s]','',doc)).split()\n",
    "    lemmas = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = token.lower()\n",
    "        lemmas.append(morph.parse(token)[0].normal_form)\n",
    "    return lemmas\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    raw_texts = {}\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    num = 0\n",
    "    for doc in corpus:\n",
    "        if type(doc) != float:\n",
    "            raw_texts[num] = normalize(doc)\n",
    "        num += 1\n",
    "    return raw_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in preprocessed:\n",
    "#     try:\n",
    "#         json_dump(preprocessed[i], 'lemmatized_corpus.json')\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(i, preprocessed[i])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in preprocessed:\n",
    "#     for j, word in enumerate (preprocessed[i]):\n",
    "#         if word == 'español':\n",
    "#             preprocessed[i][j] = 'espanol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_dump(preprocessed, 'lemmatized_corpus.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building inverted index + tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind2 = defaultdict(dict)\n",
    "# for i in tqdm(preprocessed):\n",
    "#     for word in preprocessed[i]:\n",
    "#         c = Counter(preprocessed[i])\n",
    "#         ind2[word][int(i)] = c[word] / len(preprocessed[i]) #doc num: tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind2['как']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "по непонятным причинам в json не хотело ложиться слово español. поменяла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in ind2.keys():\n",
    "#     try:\n",
    "#         json_dump(list(i), 'keys.json')\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind2['espanol'] = ind2['español']\n",
    "# del ind2['español']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'español' in ind2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_dump(ind2, 'index_tf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_tf = json_read('index_tf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = json_read('lemmatized_corpus.json', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed['14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 808555/808555 [00:02<00:00, 330288.70it/s]\n"
     ]
    }
   ],
   "source": [
    "lemma_corp = {int(key): preprocessed[key] for key in tqdm(preprocessed)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemma_corp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and BM-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing tf and idf functions for TF-IDF and BM-25 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, doc_num, indexed):\n",
    "    try:\n",
    "        return indexed[term][str(doc_num)]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def idf(term, corp, indexed):\n",
    "    try:\n",
    "        df = len(indexed[term]) \n",
    "    except KeyError:\n",
    "        df = 0\n",
    "    finally:\n",
    "        return log((len(corp) - df + 0.5) / (df + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf('перестать', lemma_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf('президентство', 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf('президентство', lemma_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avgld одна на весь корпус, поэтому посчитаем её просто в теле программы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgld = sum([len(doc) for doc in lemma_corp.values()]) / len(lemma_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avgld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing function that counts BM-25 metric for a term in a doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(term, doc_num, corp, indexed, k=2.0, b=0.75):\n",
    "    return idf(term, corp, indexed) * (tf(term, doc_num, indexed) * (k + 1)/ (tf(term, doc_num, indexed) \\\n",
    "                                + k * (1 - b + b * (len(lemma_corp[doc_num])/avgld))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "узнаем, высоко ли ранжируется слово \"президентство\" в одном из запросов про Дональда Козыря))00)0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25('президентство', 14, lemma_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fttxt_model = KeyedVectors.load('models/fasttext/model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a function for making a vector from a corpus item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(model, doc):\n",
    "    vecs = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            model.wv[word]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        vecs.append(model.wv[word])\n",
    "    return np.mean(vecs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building FastText vectors from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/808555 [00:00<?, ?it/s]C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "  1%|█                                                                         | 12029/808555 [00:32<23:17, 569.79it/s]C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 808555/808555 [21:43<00:00, 620.46it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_matr = []\n",
    "for i in tqdm(sorted(lemma_corp)):\n",
    "    ft_matr.append(vectorize(fttxt_model, lemma_corp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the cosine similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.inner(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing additional functions for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm(query, corp, indexed):\n",
    "    return [[sum([bm25(term, num, corp, indexed) for term in query]), num] for num in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(query, corp, indexed):\n",
    "    return [[sum([tf(term, num, indexed) * idf(term, corp, indexed) for term in query]), num] for num in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft(query, corp, indexed):\n",
    "    ans = []\n",
    "    for i in corp:\n",
    "        cs = cos_sim(indexed, ft_matr[i])\n",
    "        if type(cs) == np.ndarray:\n",
    "            cs = 0\n",
    "        ans.append([cs, i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(query, model):\n",
    "    if model in (tfidf, bm):\n",
    "        indexed = defaultdict(dict)\n",
    "        for term in query:\n",
    "            if term in ind_tf:\n",
    "                indexed[term] = ind_tf[term]\n",
    "            else:\n",
    "                indexed[term] = {}\n",
    "    else:\n",
    "        indexed = vectorize(fttxt_model, query)\n",
    "    return indexed\n",
    "    \n",
    "\n",
    "def search(query, model):\n",
    "    tfs = index(query, model)\n",
    "    return sorted(model(query, corp=lemma_corp, indexed=tfs), reverse=True)[:10]\n",
    "    \n",
    "    \n",
    "def output(query, model):\n",
    "    l_query = normalize(query)\n",
    "    return [corpus[num] for num in [doc[1] for doc in search(l_query, model)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding it to the Flask functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def form():\n",
    "    if request.args:\n",
    "        query = request.args['query']\n",
    "        if request.args['model'] == 'tfidf':\n",
    "            model = tfidf\n",
    "        elif request.args['model'] == 'bm':\n",
    "            model = bm\n",
    "        elif request.args['model'] == 'fasttext':\n",
    "            model = ft\n",
    "        try:\n",
    "            outlist = output(query, model)\n",
    "        except Exception as e:\n",
    "            outlist = e.split(' ')  # обработка \n",
    "        return render_template('result.html', output=outlist)\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2af1c2e2e6b4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2af1c2e2e6b4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Implementing search\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [25/Oct/2019 10:31:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2019 10:32:00] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [25/Oct/2019 10:33:44] \"GET /?query=в+чем+смысл+жизни&model=tfidf HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Oct/2019 10:34:55] \"GET /?query=в+чем+смысл+жизни&model=bm HTTP/1.1\" 200 -\n",
      "C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\Admin\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "127.0.0.1 - - [25/Oct/2019 10:37:06] \"GET /?query=в+чем+смысл+жизни&model=fasttext HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
